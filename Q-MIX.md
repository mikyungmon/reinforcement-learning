# QMIX #

- 1990년대 초반에 제안된 IQL(Independent Q-Learning) 알고리즘과 최근 DQN을 적용한 멀티 에이전트 심층 강화학습 알고리즘에서는 다수의 에이전트 각각이 Q러닝 알고리즘을 이용하여 자신이 부분적으로 관측한 로컬 상황을 기반으로 자신의 행동을 선택할 수 있는 로컬 Q함수를 다른 에이전트에 대한 정보 없이 독립적으로 학습하는 방식을 제안하고 있다.

- 이러한 완전 분산형 구조의 경우 학습과 탐색을 동시에 수행하고 있는 다른 에이전트들을 환경의 일부로 간주하기 때문에 주 에이전트 관점에서는 Q러닝 알고리즘이 수렴하기 위해 필요한 마코프 가정을 만족하지 못하게 되어 **안정적인 학습이 어렵다**는 단점을 가진다.

- 반대로, 에이전트 각각의 로컬 Q함수를 학습하는 대신에 에이전트들의 모든 로컬 관측 상황과 전역 환경 상태를 고려하여 모든 에이전트들의 공동행동이 가져올 누적 보상의 기댓값을 예측하는 하나의 전역 Q함수를 학습하는 방식이 가능하지만 이러한 완전 집중형 학습방식은 에이전트의 수가 증가함에 따라 공동 행동 공간이 기하급수적으로 증가하기 때문에 에이전트의 수에 제약이 따른다.

- **QMIX는 이 두가지 방식을 절충하여 다수의 에이전트가 각각 자신이 부분적으로 관측한 로컬 상황에서 개별 행동의 가치를 예측해주는 로컬 Q함수와 모든 로컬 Q함수의 출력값들과 에이전트들의 개별 행동에 대한 전역 환경 상태를 모두 종합하여 에이전트들의 공동행동에 대한 가치를 예측하는 전역 Q함수를 모두 사용하는 새로운 멀티 에이전트 학습 방식을 제안한다.**

*QMIX 알고리즘은 로컬 Q함수를 근사하는 에이전트 신경망과 다른 에이전트들의 행동을 고려하는 전역 Q함수를 근사하는 mixing 신경망을 최적의 공동 행동 학습을 목표로 end to end학습하기 때문에 기존의 완전 분리형 방식보다 안정적인 학습이 가능하다.*

### 제약조건 ###

1) 모든 에이전트는 부분 관측 정보를 이용한다.

2) 각 에이전트의 부분 관측 정보는 서로 공유하지 않는다.

3) 보상 또한 각 에이전트의 개개인이 아닌 팀 보상으로 주어진다.

### QMIX 아키텍처 ###

![image](https://user-images.githubusercontent.com/66320010/114545500-03a3e700-9c97-11eb-8454-88ea807f524d.png)

    - 𝑄_𝑎 : 에이전트 a의 Q값
    - 𝑄_𝑡^𝑎 : 에이전트 a의 t-step의 observation
    - 𝑢_𝑡^𝑎 : t-step에서 에이전트 a의 joint-action
    - 𝑄_𝑡𝑜𝑡 : 각 에이전트들의 Q값 종합체
- (c)는 각 에이전트의 아키텍처 => 각자의 부분 관측값(observation)과 joint-action을 넣어 Q값 산출

- 이렇게 산출된 Q값을 (b)와 같이 mixing 네트워크에 넣는다. mixing 네트워크는 centralize learning 역할을 한다. 

    **=> 즉, 각 에이전트의 Q값과 전체 state정보를 받아 산출하는 구조**
    
- mixing 네트워크는 각 에이전트의 Q값과 state를 비선형 신경망에 적용하여 𝑄_𝑡𝑜𝑡를 최적화 시킨다 ( 산출된 𝑄_𝑡𝑜𝑡값을 이용해 target 업데이트를 진행 ) -> 이 때 Q함수는 monotonic해야한다.

## 요약 ##

- QMIX는 기존 IQL기법에서 centrailze Learning과, Reward Shape(각 agent의 policy에 대한 보상을 간접적으로 추정하는 기법)이 도입된 알고리즘

- mixing 네트워크가 centralize learning을 담당하며 이를 통해 𝑄_𝑡𝑜𝑡 산출

- 𝑄_𝑡𝑜𝑡값 사용하여 reward shape, 즉 각 에이전트의 Q값을 평가

  ***정리하면 각 에이전트의 Q값을 신경망으로 총합하고 그 값을 통하여 각 에이전트를 평가하는 기법***
