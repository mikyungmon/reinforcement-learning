# 강화학습 #

**강화학습**이란?

주어진 환경과 상호작용하여 점수를 얻는 방향으로 성장하는 머신러닝 분야이다.

사람은 누가 가르쳐주지 않았는데도 걷는 방법을 익힌다. 일어섰다가 넘어지는 시행착오를 겪으며 좋은 피드백을 받을 쪽으로 최적화하며 성장한다.

이처럼 강화학습 모델도 주어진 환경에서 시행착오를 겪으며 좋은 피드백을 받는 쪽으로 최적화하며 성장하는 것이다.

![image](https://user-images.githubusercontent.com/66320010/113860883-81707a00-97e1-11eb-97ab-2cca414b846c.png)

강화학습은 크게 4가지 요소로 나눌 수 있다.

![image](https://user-images.githubusercontent.com/66320010/113860392-f0010800-97e0-11eb-88a5-a1688313678e.png)

1) 에이전트 - 인공지능 플레이어
2) 환경 - 에이전트가 솔루션을 찾기 위한 무대
3) 행동 - 에이전트가 환경 안에서 시행하는 시행착오
4) 보상 - 에이전트의 행동에 따른 점수 혹은 결과

강화학습의 최종 목표는 환경과 상호작용을 하는 임의의 에이전트를 학습시키는 것이다. 

**에이전트(agent)** 는 **상태(state)** 라고 부르는 다양한 상황 안에서 **행동(action)** 을 취하며 조금씩 학습해 나간다.

에이전트가 취한 행동은 그에 대한 응답으로 양(+)이나 음(-), 또는 0의 **보상(reward)** 를 돌려받는다.

## Markov Decision Processes (마코프 결정 프로세스) ## 

모든 상태(State)는 그 직전의 상태와, 그 상태에서 Agent가 선택한 행동이 만든 직접적인 결과이다. 직전의 상태는 또한 그 이전의 상태와 그 이전의 상태에 한 행동의 결과이고, ... 이렇게 계속하다보면 처음 시작했던 시점까지 올라갈 수 있다. 

이렇게 연결되어 있는 모든 단계들과 그 순서는 현재 상태를 결정짓는 어떤 정보를 담고 있을 것이고, 그에 따라 지금 순간에 Agent가 어떤 행동을 선택해야 하는지에 대해서도 직접적인 영향을 미친다. 

즉, 모든 정보들을 다 활용할 수 있다면 Agent가 어떤 선택을 해야하는지 결정하는 데에 큰 도움이 될 것이다.

하지만 이런 식이라면 한 가지 문제가 발생한다. 바로 Agent가 한 단계씩 나아갈 때마다, 점점 더 많은 정보들이 저장되어야 하고 그 양은 계속 늘어난다는 점이다. 

이렇게 많은 양의 데이터를 다루면서 연산한다는 것은 쉽지 않은 일이다. 

이 문제를 해결하기 위해서, 우리는 모든 상태가 Markov State에 해당한다고 가정한다. 

**Markov State** 란, "모든 상태는 오직 그 직전의 상태와 그 때 한 행동에 대해서만 의존한다"는 가정이다. 

즉, 직전 상태 말고 그보다 더 이전의 상태들에 대해서는 고려하지 않아도 되는 것이다. 바로 한 가지 간단한 예로 Tic-Tac-Toe 게임이 있다. 

![image](https://user-images.githubusercontent.com/66320010/113866190-f941a300-97e7-11eb-8947-0cec60dd1408.png)

위쪽과 아래쪽, 두 게임 모두 결과적으로 맨 오른쪽을 보면 같은 상태에 도달했지만, 그 과정은 다르다. 

또한, 두 게임 모두 마지막 상태에서 파란색 플레이어는 맨 오른쪽 위의 칸에 다음 수를 둬서 상대방을 잡지 못하면 지게 된다. 

이렇게 Tic-Tac-Toe 게임은 **우리가 매 순간 행동을 선택하는 데에 있어서 고려해야 하는 것은 오직 직전의 상태일 뿐, 그 이전에 어떤 경로를 통해서 그 상태에 도달했는지는 고려하지 않아도 된다.** 

이러한 가정이 바로 Markov assumption인데, 이 가정을 이용하면서 기억해야 할 점은 상황이 진행됨에 따라 데이터를 잃게 된다는 것이다. 

위와 같이 단순해서 Markov assumption이 정확히 맞아 떨어지는 Tic-Tac-Toe는 문제가 없지만, 더 복잡한 게임인 체스나 바둑같은 게임에서는 게임이 진행되어가는 경로에 상대방의 의도나 전략 등의 정보들이 담겨있을 수 있기 때문에 문제가 될 수 있다.

하지만, 그럼에도 불구하고 Markov assumption은 장기 전략을 짜는데에 가장 기본적인 가정으로 많이 쓰인다. 

## The Bellman Equation ( 벨만 방정식 ) ##

![image](https://user-images.githubusercontent.com/66320010/113866899-d1067400-97e8-11eb-9608-8889574e7df5.png)

상태 s에서 행동 a를 취할 때 받을 수 있는 모든 보상의 총 합 Q(s,a)는 , 현재 행동을 취해서 받을 수 있는 **즉각 보상**과 미래에 받을 **미래보상의 최대값**의 합으로 계산할 수 있다.

우변의 r(s,a)는 현재 상태 s에서 행동a를 취했을 때 받을 수 있는 즉각 보상값을 나타내며 s'는 현재 상태 s에서 행동 a를 취해 도달하는 바로 다음의 상태로 max Q(s',a)는 다음상태 s'에서 받을 수 있는 보상의 최대값이다. 

또한 그 앞에 붙어있는 γ는 **할인율**이라고 부르는 값으로 미래 가치에 대한 중요도를 조절한다. γ값이 **커질수록 미래에 받을 보상**에 더 큰 가치를 두는 것이고 **작아질수로 즉각보상을 더 중요하게 고려하는 것**이다.


**=> 이 값을 최대화 할 수 있는 행동을 선택해내는 것이 Agent의 목표이다.**

### Value Fuction (가치 함수) ###

![image](https://user-images.githubusercontent.com/66320010/113871031-6efc3d80-97ed-11eb-861b-05baa1927378.png)

순차적인 결정문제를 풀기 위해서는 가치함수(Value Fuction)을 정의해야한다.

**가치함수**는 현재 정책(policy)를 따라갔을 때 얻는 예측 보상의 총 합을 의미한다. -> 이 때 현재 보상의 추세인 감가율을 고려하여 미래 보상을 예측한다.

에이전트는 가치함수를 통해서 보상의 합을 최대로 한다는 목표에 얼마나 다가갔는지를 판단한다. 가치함수에 대한 방정식이 **벨만 방정식**이다.

## 강화학습 알고리즘 분류 ##

![image](https://user-images.githubusercontent.com/66320010/113862543-7d455c00-97e3-11eb-94dd-9974a2e3fc0b.png)

**1. model-free vs. model-based**

강화학습 알고리즘의 첫번째 구분은 environment(환경)에 대한 model의 존재 여부이다. 

*( model이란? 환경의 다음 state와 보상이 어떨지에 대한 에이전트의 예상. state model과 reward model로 나눌 수 있음 )*

Model을 갖는 것은 장점과 단점이 있다.

Model을 갖는 것의 장점은 Planning(계획)을 가능하게 한다는 것이다. 즉, 자신의 action에 따라서 environment가 어떻게 바뀔지 안다면 실제로 행동하기 전에 미리 변화를 예상해보고 최적의 행동을 계획하여 실행할 수 있다. 

이와 같은 계획이 가능하다면 agent는 훨씬 효율적으로 행동할 수 있을 것이다.

Model을 갖는 것의 단점은 environment의 정확한 model은 보통 알아내기가 어렵거나 불가능하다는 점이다. 혹시라도 Model이 environment를 제대로 반영하지 않는다면 이 오류는 그대로 agent의 오류로 이어지게 된다. 정확한 model을 만드는 것은 좋은 agent를 만드는 것만큼 또는 더 어려울 수 있다.

Model을 사용하는 agent를 model-based라고 부르고 그렇지 않은 agent를 model-free라고 부른다.

Model-based agent는 다시 모델이 주어져 있는지 아니면 학습 대상인지에 따라 구분할 수 있다.

**2.Value-based vs. policy-based**

강화학습 알고리즘의 두번째 구분은 value function과 policy의 사용 여부이다.

만약 value function(가치 함수)이 완벽하다면 최적의 policy는 자연스럽게 얻을 수 있다 -> 각 state에서 가장 높은 value를 주는 action만을 선택하면 되기 때문에 

이를 implicit (암묵적인) policy라고 한다. Value function 만을 학습하고 policy는 암묵적으로만 갖고 있는 알고리즘들이 있다. 이를 value-based agent라고 부른다. DQN 등이 여기에 해당한다.

반대로 Policy가 완벽하다면 Value function은 굳이 필요하지 않다. 결국 Value function은 policy를 만들기 위해 사용되는 중간 계산일 뿐이기 떄문에.

이처럼 Value function이 없이 policy만을 학습하는 agent를 policy-based라고 부른다. Policy Gradient 등이 여기에 해당한다.

Value-based agent는 데이터를 더 효율적으로 활용할 수 있다는 장점이 있다. 이에 비해 policy-based agent는 원하는 것에 직접적으로 최적화를 하기 때문에 더욱 안정적으로 학습된다는 장점이 있다.

두 극단적인 케이스만 있는 것은 아니다. Value function과 Policy를 모두 갖고 있는 agent도 있다. 이를 Actor-Critic agent라고 부른다.



