# 강화학습 #

**강화학습**이란?

주어진 환경과 상호작용하여 점수를 얻는 방향으로 성장하는 머신러닝 분야이다.

사람은 누가 가르쳐주지 않았는데도 걷는 방법을 익힌다. 일어섰다가 넘어지는 시행착오를 겪으며 좋은 피드백을 받을 쪽으로 최적화하며 성장한다.

이처럼 강화학습 모델도 주어진 환경에서 시행착오를 겪으며 좋은 피드백을 받는 쪽으로 최적화하며 성장하는 것이다.

![image](https://user-images.githubusercontent.com/66320010/113860883-81707a00-97e1-11eb-97ab-2cca414b846c.png)


강화학습은 크게 4가지 요소로 나눌 수 있다.

![image](https://user-images.githubusercontent.com/66320010/113860392-f0010800-97e0-11eb-88a5-a1688313678e.png)

1) 에이전트 - 인공지능 플레이어
2) 환경 - 에이전트가 솔루션을 찾기 위한 무대
3) 행동 - 에이전트가 환경 안에서 시행하는 시행착오
4) 보상 - 에이전트의 행동에 따른 점수 혹은 결과

## 강화학습 알고리즘 분류 ##

![image](https://user-images.githubusercontent.com/66320010/113862543-7d455c00-97e3-11eb-94dd-9974a2e3fc0b.png)

**1. model-free vs. model-based**

강화학습 알고리즘의 첫번째 구분은 environment(환경)에 대한 model의 존재 여부이다. 

*( model이란? 환경의 다음 state와 보상이 어떨지에 대한 에이전트의 예상. state model과 reward model로 나눌 수 있음 )*

Model을 갖는 것은 장점과 단점이 있다.

Model을 갖는 것의 장점은 Planning(계획)을 가능하게 한다는 것이다. 즉, 자신의 action에 따라서 environment가 어떻게 바뀔지 안다면 실제로 행동하기 전에 미리 변화를 예상해보고 최적의 행동을 계획하여 실행할 수 있다. 
이와 같은 계획이 가능하다면 agent는 훨씬 효율적으로 행동할 수 있을 것이다.

Model을 갖는 것의 단점은 environment의 정확한 model은 보통 알아내기가 어렵거나 불가능하다는 점이다. 혹시라도 Model이 environment를 제대로 반영하지 않는다면 이 오류는 그대로 agent의 오류로 이어지게 된다. 정확한 model을 만드는 것은 좋은 agent를 만드는 것만큼 또는 더 어려울 수 있다.

Model을 사용하는 agent를 model-based라고 부르고 그렇지 않은 agent를 model-free라고 부른다.

Model-based agent는 다시 모델이 주어져 있는지 아니면 학습 대상인지에 따라 구분할 수 있다.

**2.Value-based vs. policy-based**

강화학습 알고리즘의 두번째 구분은 value function과 policy의 사용 여부이다.

만약 value function(가치 함수)이 완벽하다면 최적의 policy는 자연스럽게 얻을 수 있다 -> 각 state에서 가장 높은 value를 주는 action만을 선택하면 되기 때문에 

이를 implicit (암묵적인) policy라고 한다. Value function 만을 학습하고 policy는 암묵적으로만 갖고 있는 알고리즘들이 있다. 이를 value-based agent라고 부릅니다. DQN 등이 여기에 해당한다.

반대로 Policy가 완벽하다면 value function은 굳이 필요하지 않다. 결국 value function은 policy를 만들기 위해 사용되는 중간 계산일 뿐이기 떄문에.

이처럼 value function이 없이 policy만을 학습하는 agent를 policy-based라고 부른다. Policy Gradient 등이 여기에 해당한다.

Value-based agent는 데이터를 더 효율적으로 활용할 수 있다는 장점이 있다. 이에 비해 policy-based agent는 원하는 것에 직접적으로 최적화를 하기 때문에 더욱 안정적으로 학습된다는 장점이 있다.

두 극단적인 케이스만 있는 것은 아니다. Value function과 Policy를 모두 갖고 있는 agent도 있다. 이를 Actor-Critic agent라고 부른다.



