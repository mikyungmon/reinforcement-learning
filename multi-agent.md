# 멀티 에이전트 ( multi-agent ) #

**멀티 에이전트** 란 ?

  - 주어진 환경에서 두 개 이상의 에이전트가 협업 또는 경쟁을 통해 높은 보상을 얻을 수 있는 행동 정책(policy)를 학습하는 기술로 정의할 수 있다.

  - 기존 싱글 에이전트 중심의 강화학습 기술에서 고려하고 있는 **탐색-이용 딜레마(Explore-exploit dilemma), 부분 관측 가능성(Partial observability)** 에 따른 문제들뿐만 아니라 멀티 에이전트 환경이 갖는 고유의 비정상성 특성과 에이전트 간의 **신뢰할당(Credit-assignment) 문제** 까지 추가로 고려해야 하기 때문에 멀티 에이전트 학습은 더욱 복잡하고 어려운 것으로 알려져 있다.

  - 최근 심층신경망을 이용하여 에이전트의 정책 함수나 가치 함수를 근사하는 심층 강화학습의 발전에 힘입어 이를 기반으로 하는 다양한 멀티 에이전트 심층 강화학습 알고리즘들이 제안되고 있다. 

  - 멀티 에이전트 심층 강화학습 알고리즘 역시 최적의 행동가치함수(Action-value function) 학습을 목표로 하는 Q-러닝 계열과 정책 함수를 경사상승법을 이용하여 직접 학습하는 정책 경사 계열로 구분할 수 있으며, 대표적인 알고리즘으로는 Q-러닝 계열의 QMIX 알고리즘과 정책 경사 계열의 MADDPG 알고리즘 등이 있다.

## 멀티 에이전트 강화학습 알고리즘 ##

1. 에이전트 간의 관계 모델링

   - 멀티 에이전트 강화학습에서는 싱글 에이전트 강화학습과는 달리, 협업 또는 경쟁에 대한 다수의 에이전트의 최적 행동을 찾아야한다.   
   => 그래서 다른 에이전트들에 대한 영향이나 에이전트간의 관계가 정책이나 행동-가치 함수에 반영되어야 한다.
      
   - 다른 에이전트에 대한 영향을 멀티 에이전트 강화학습에서 본격적으로 고려하기 시작한 알고리즘은 MADDPG( Multi-Agent Deep Deterministic Policy Gradient )이다.


    ### [ 1. MADDPG ( Multi-Agent Deep Deterministic Policy Gradient ) ] ###

    - **MADDPG**는 싱글 에이전트 강화학습에서의 대표적인 정책 경사기반 방법인  DDPG(DeepDeterministic Policy Gradient)알고리즘을 기반으로 하는 알고리즘으로서 다른 에이전트의 상태와 행동을 직접 행동-가치 함수의 입력으로 사용하여, 다른 에이전트의 영향을 고려하는 정책을 찾는 알고리즘이다.

    - 모든 에이전트의 상태와 행동을 입력으로 사용하기 때문에 에이전트의 수가 증가하거나 에이전트의 상태와 행동의 수가 늘어나면 정책 경사를 구하기 위한 목적 함수가 기하급수적으로 복잡해진다.

    ### [ 2. MF-RL(Mean Field Reinforcement Learning) ] ###

    - 특정 에이전트의 정책을 계산할 때 해당 에이전트의 주변 범위 내의 에이전트들에 대한 **평균 행동**을 계산하여, 해당 에이전트의 정책 경사의 목적함수의 입력 또는 행동-가치 함수의 입력으로 사용하는 알고리즘이다.

    - 다른 에이전트의 영향을 주변 에이전트의 평균 행동이라는 하나의 인자로 축약해서 표현하였기 때문에, 에이전트의 수가 많더라도 에이전트 간의 영향의 범위가 넓지 않고, 행동의 복잡도가 크지 않은 환경에서는 우수한 정책 또는 행동-가치 함수를 찾을 수 있으나, 에이전트 간의 영향의 범위가 넓은 시나리오에서는 적용에 한계가 있다.

    ### [ 3. MAAC( Multi-Actor-Attention-Critic) ] ###

   - 인공 신경망 형태로 구성된 어텐션 메커니즘을 이용하여 상황별로 에이전트 간의 중요도를 계산하여 정책 경사의 목적함수의 입력으로 사용한 알고리즘이며, 싱글 에이전트 강화학습 알고리즘 중 Soft Actor-Critic 알고리즘을 기반으로 구성 되었다.
      
    ### [ 4. HAMA(Hierarchical graph Attention-based Multi-agent Actor-critic) ] ###

   - 에이전트의 특성에 따라 그룹을 구성하여 그래프 어텐션 메커니즘을 기반으로 그룹별 어텐션을 계산하고 정책 경사에 적용한 알고리즘이다.
  
    - 에이전트별로 특성이 명확히 구분되어 그룹을 만들기 쉬운 환경에서는 HAMA 알고리즘이 기존 알고리즘 대비 높은 성능을 보였고, 그룹별로 어텐션을 계산하기 때문에 그룹 내의 에이전트 수가 늘어나더라도 성능 저하 없이 알고리즘이 작동했다.


