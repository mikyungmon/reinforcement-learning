# 멀티 에이전트 ( multi-agent ) #

**멀티 에이전트** 란 ?

  - 주어진 환경에서 두 개 이상의 에이전트가 협업 또는 경쟁을 통해 높은 보상을 얻을 수 있는 행동 정책(policy)를 학습하는 기술로 정의할 수 있다.

  - 기존 싱글 에이전트 중심의 강화학습 기술에서 고려하고 있는 **탐색-이용 딜레마(Explore-exploit dilemma), 부분 관측 가능성(Partial observability)** 에 따른 문제들뿐만 아니라 멀티 에이전트 환경이 갖는 고유의 비정상성 특성과 에이전트 간의 **신뢰할당(Credit-assignment) 문제** 까지 추가로 고려해야 하기 때문에 멀티 에이전트 학습은 더욱 복잡하고 어려운 것으로 알려져 있다.

  - 최근 심층신경망을 이용하여 에이전트의 정책 함수나 가치 함수를 근사하는 심층 강화학습의 발전에 힘입어 이를 기반으로 하는 다양한 멀티 에이전트 심층 강화학습 알고리즘들이 제안되고 있다. 

  - 멀티 에이전트 심층 강화학습 알고리즘 역시 최적의 행동가치함수(Action-value function) 학습을 목표로 하는 Q-러닝 계열과 정책 함수를 경사상승법을 이용하여 직접 학습하는 정책 경사 계열로 구분할 수 있으며, 대표적인 알고리즘으로는 Q-러닝 계열의 QMIX 알고리즘과 정책 경사 계열의 MADDPG 알고리즘 등이 있다.

## 멀티 에이전트 강화학습 알고리즘 ##

1. 에이전트 간의 관계 모델링

   - 멀티 에이전트 강화학습에서는 싱글 에이전트 강화학습과는 달리, 협업 또는 경쟁에 대한 다수의 에이전트의 최적 행동을 찾아야한다.   
   => 그래서 다른 에이전트들에 대한 영향이나 에이전트간의 관계가 정책이나 행동-가치 함수에 반영되어야 한다.
      
   - 다른 에이전트에 대한 영향을 멀티 에이전트 강화학습에서 본격적으로 고려하기 시작한 알고리즘은 MADDPG( Multi-Agent Deep Deterministic Policy Gradient )이다.


    ### [ 1. MADDPG ( Multi-Agent Deep Deterministic Policy Gradient ) ] ###

    - **MADDPG**는 싱글 에이전트 강화학습에서의 대표적인 정책 경사기반 방법인  DDPG(DeepDeterministic Policy Gradient)알고리즘을 기반으로 하는 알고리즘으로서 다른 에이전트의 상태와 행동을 직접 행동-가치 함수의 입력으로 사용하여, 다른 에이전트의 영향을 고려하는 정책을 찾는 알고리즘이다.

    - 모든 에이전트의 상태와 행동을 입력으로 사용하기 때문에 에이전트의 수가 증가하거나 에이전트의 상태와 행동의 수가 늘어나면 정책 경사를 구하기 위한 목적 함수가 기하급수적으로 복잡해진다.

    ### [ 2. MF-RL(Mean Field Reinforcement Learning) ] ###

    - 특정 에이전트의 정책을 계산할 때 해당 에이전트의 주변 범위 내의 에이전트들에 대한 **평균 행동**을 계산하여, 해당 에이전트의 정책 경사의 목적함수의 입력 또는 행동-가치 함수의 입력으로 사용하는 알고리즘이다.

    - 다른 에이전트의 영향을 주변 에이전트의 평균 행동이라는 하나의 인자로 축약해서 표현하였기 때문에, 에이전트의 수가 많더라도 에이전트 간의 영향의 범위가 넓지 않고, 행동의 복잡도가 크지 않은 환경에서는 우수한 정책 또는 행동-가치 함수를 찾을 수 있으나, 에이전트 간의 영향의 범위가 넓은 시나리오에서는 적용에 한계가 있다.

    ### [ 3. MAAC( Multi-Actor-Attention-Critic) ] ###

    - 인공 신경망 형태로 구성된 어텐션 메커니즘을 이용하여 상황별로 에이전트 간의 중요도를 계산하여 정책 경사의 목적함수의 입력으로 사용한 알고리즘이며, 싱글 에이전트 강화학습 알고리즘 중 Soft Actor-Critic 알고리즘을 기반으로 구성 되었다.
      
    ### [ 4. HAMA(Hierarchical graph Attention-based Multi-agent Actor-critic) ] ###

   - 에이전트의 특성에 따라 그룹을 구성하여 그래프 어텐션 메커니즘을 기반으로 그룹별 어텐션을 계산하고 정책 경사에 적용한 알고리즘이다.
  
    - 에이전트별로 특성이 명확히 구분되어 그룹을 만들기 쉬운 환경에서는 HAMA 알고리즘이 기존 알고리즘 대비 높은 성능을 보였고, 그룹별로 어텐션을 계산하기 때문에 그룹 내의 에이전트 수가 늘어나더라도 성능 저하 없이 알고리즘이 작동했다.

2. 에이전트 간의 신뢰할당

    - 다수의 에이전트가 운용될 때, 환경으로부터 에이전트마다 별도의 보상을 부여받는 때도 있지만, 문제에 따라서는 모든 에이전트의 행동에 대해 하나의 **공동 보상**만 제공되기도 한다.

    - 이때, 환경으로부터 얻어진 보상이 어떤 에이전트의 기여에 의한 것인지를 명확히 구분하고 정량화 할 수 있으면 에이전트마다 보상에 대한 기여를 정확히 분배할 수 있다.

        => 에이전트에 대한 기여도를 분배하는 문제를 **신뢰할당** 문제라고 함
        
    ### [ 1. COMA(COunterfactual Multi-Agent policy gradient) ] ###
    
    - COMA는 멀티 에이전트 환경에서의 신뢰할당 문제를 해결하기 위해 새로운 형태의 **이득함수(Advantage Function)** 를 제안하였다.

    - 기존의 이득 함수는 행동-가치 함수에서 상태-가치 함수 (State-Value Function)와의 차이를 통해 계산되나, COMA에서는 상태-가치 함수 대신 다른 모든 에이전트의 행동이 고정된 상태에서 특정 에이전트의 행동에 대한 행동-가치 함수의 평균값을 이용 해 이득 함수를 계산하게 된다.
    
    ***공동 행동-가치 함수를 유틸리티 함수(에이전트의 행동-가치 함수 역할을 하는)로 분해하는 것을 가치 함수 분해라 한다. 가치 함수 분해가 성공적으로 수행되면 실행 단계에서도 에이전트는 자신의 유틸리티 함수 만을 이용해 공동 행동-가치 함수가 높은 행동을 찾을 수 있다. 대표적 가치 함수 분해 연구로는 VDN, QMIX, QTRAN 알고리즘이 있다.***   
    
    ### [ 2. VDN(Value Decomposition Networks) ] ###
    
    -  각 에이전트의 유틸리티 함수의 합으로 공동 행동-가치 함수를 구성한다.
    
    ### [ 3. QMIX ] ### 
    
    - VDN에 비해 더 일반적인 형태로 유틸리티 함수와 공동 행동-가치 함수의 관계를 구성하기 위해 QMIX에서는 유틸리티 함수를 입력으로 하는 단층의 인공 신경망으로 공동 행동-가치 함수를 계산하는 Mixing network를 제안한다.
    
    - QMIX는 VDN에 비해 복잡한 시나리오의 문제에서도 더 높은 성능을 보임을 확인되었다.
    
    ### [ 4. QTRAN ] ###
    
    - 하나의 에이전트가 자신의 유틸리티 함수가 감소 하는 행동을 선택하는 것이 모든 에이전트 관점에서 유리한 경우에는 QMIX로 최적의 정책을 얻지 못한다.
    
      => 이렇게 더 일반적인 형태의 문제를 해결 하기 위해서 QTRAN 알고리즘이 제안되었다.
    
    ### [ 5. LIIR(Learning Individual Intrinsic Reward) ] ### 
    
    - **LIIR**은 정책 경사기반의 방법으로서 정 책과 Critic에 대한 인공 신경망 외에 각 에이전트 에 대한 개별 보상을 위한 별도의 인공 신경망을 구성하였다.
     
      => 여기서 개별 보상은 환경으로부터 주어지는 외적 보상(Extrinsic Reward)이 아닌, 에이전트가 스스로 생성하는 내적 보상(Intrinsic Reward)이다.
    
    - 내적 보상의 형태로 에이전트마다 개별적으로 보상을 부여하기 때문에 높은 성능의 정책을 찾을 수 있고, 기존의 알고리즘으로 찾기 어려운 다양한 패턴의 정책을 찾을 수 있다는 장점이 있다는 것이다.
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
