# Meta-RL # 

**Meta-RL** 이란? 

-> 강화학습 task에 meta learning을 적용한 것이다.

여기서 **meta learning**이란 머신러닝 실험에 대한 메타 데이터에 자동 학습 알고리즘이 적용되는 머신러닝의 하위분야로써, 과거의 학습 경험을 토대로 모델이 새로운 task를 잘 습득할 수 있게 만든 것이다.

예를 들어 머신러닝은 좋은 모델을 만들기 위해서는 일반적으로 수많은 샘플을 활용해 학습시키는 것이 필요하다. 하지만 고양이나 새를 많이 보지 않은 사람이라고 할지라도 그것을 빠르게 구분할 수 있다. 

머신러닝에도 이와 같이 적은 샘플만 가지고도 새로운 개념과 기술을 빠르게 학습하는 것이 가능할까? 이것이 바로 본질적으로 **meta learning**이 풀고하자는 문제점이다.

![image](https://user-images.githubusercontent.com/66320010/117289420-850e2400-aea7-11eb-8e5c-139ecbb19149.png)

- meta learning은 학습을 수행하며 만들어진 학습에 대한 'meta data'를 활용한다.

- 새로운 task에 대한 학습 결과를 예측하고 최적의 'meta data'를 도출하는 것을 목표로 한다.

- meta learning은 3가지로 나눌 수 있다.

  1) Model Evaluation ->  "**다른 task**에 대해 학습시켰을 때의 학습 결과를 토대로 **신규 task**에 대한 결과를 예측하고, 하이퍼파라미터를 추천하자!"
  2) Meta Feature
  3) Prior Model

**Meta-RL은 경험하지 못한 task에 대해서 빠르고 효율적으로 해결할 수 있는 agent를 개발함으로써 "meta learn" 강화학습 task를 해결하는 경우에 대해서 다룬다.**

## 관련 논문 preview ##

**Guided Meta-Policy Search(2020)**   

논문 링크 : https://arxiv.org/pdf/1904.00956.pdf

- **Abstract**

  강화학습 알고리즘은 복잡한 작업에 대해 유망한 결과를 보여 주었지만 처음부터 학습하기 때문에 종종 비실용적인 수의 샘플이 필요하다. 

  Meta-RL은 이전 작업의 경험을 활용하여 새로운 작업을보다 신속하게 해결함으로써 이러한 문제를 해결하는 것을 목표로 한다. 

  그러나 실제로 이러한 알고리즘은 일반적으로 meta learning 과정 중에 많은 양의 정책에 대한(on-policy) 경험을 필요로 하므로 많은 문제에서 사용하기에는 비실용적이다. 

  이를 위해 우리는 정책을 벗어난(off-policy) 개별 학습자가 개별 meta learning 과제를 해결한 다음, 이러한 솔루션을 단일 meta-learner로 통합 할 수 있는 연합 방식으로 강화 학습 절차를 학습 할 것을 제안한다. 

  중앙 meta-learner는 개별 작업에 대한 솔루션을 모방하여 학습하므로 표준 meta RL 문제 설정 또는 일부 또는 모든 작업에 예제 데모가 제공되는 하이브리드 설정을 수용 할 수 있다. 

  전자는 메타 교육 중에 상당한 양의 정책 데이터없이 이전 작업에 대해 학습 한 정책을 활용할 수있는 접근 방식을 제공하는 반면 후자는 데모를 제공하기 쉬운 경우에 특히 유용하다. 

  여러 연속 제어 meta-RL 문제에서, 우리는 이전 작업과 비교하여 meta-RL 샘플 효율성이 현저히 개선되었으며 시각적 관찰을 통해 도메인으로 확장할 수있는 능력을 보여준다. 

- **Introduction**

  **meta learning**은 광범위한 작업에서 이전 경험을 사용하여 새로운 작업의 학습을 크게 가속화 할 수있는 유망한 접근 방식이다. 

  **메타 강화 학습(Meta RL)** 은 이전 경험을 바탕으로 환경과의 상호 작용을 몇 번만 수행하여 시행 착오를 통해 새로운 행동을 학습하는 맥락에서 이 문제를 특별히 고려한다. 

  효과적인 meta-RL 알고리즘을 구축하는 것은 에이전트가 각각의 새로운 객체와 목표에 대해 처음부터 배우지 않고도 새로운 방식으로 새로운 객체를 조작할 수있는 것과 같이 유연한 에이전트를 구축하는데 중요하다. 

  이러한 방식으로 이전 경험을 재사용 할 수 있다는 것은 아마도 지능의 근본적인 측면일 것이다.

  에이전트가 meta-RL을 통해 적응할 수 있도록하는 것은 다양하고 역동적인 환경이 있는 실제 상황에서 행동을 획득할 때 특히 유용하다. 

  그러나 **최근의 발전에도 불구하고 현재의 meta-RL 방법은 상대적으로 낮은 차원의 연속 제어 작업 및 개별 작업 명령을 사용한 탐색과 같은 단순한 영역으로 제한되는 경우가 많다.**  

  최적화 안정성과 샘플 복잡성은 이러한 방법의 meta learning 단계에서 주요 과제이며 일부 최근 기술은 표 MDP에서 meta learning을 위해 최대 2억 5천만번의 전환이 필요하며 이는 일반적으로 1초 이내에 분리하여 해결해야한다. 
  
  해당 논문에서는 이 작업에서 다음과 같은 관찰을 한다. 메타 강화 학습(Meta RL)의 목표는 빠르고 효율적인 강화 학습 절차를 습득하는 것이지만 이러한 절차 자체는 강화 학습을 통해 직접 습득 할 필요가 없다. 

  대신 메타 수준에서 감독을 제공하기 위해 훨씬 더 안정적이고 효율적인 알고리즘을 사용할 수 있다. 

  이 작업에서 우리는 감독된(지도된) 모방 학습을 사용하는 것이 실용적인 선택임을 보여준다. 

  메타 강화 학습 알고리즘은 강화를 통해 작업을 빠르게 학습 할 수 있는 능력을 최적화하는 동시에 전문가 조치의 형태로 메타 학습 중에 보다 직접적인 감독을 받을 수 있다. 

  결정적으로 이러한 전문가 정책은 표준 강화 학습 방법에 의해 자동으로 생성될 수 있으므로 감독에 대한 추가 가정이 실제로 필요하지 않다. 

  메타 강화 학습과 함께 사용하기 어려운 매우 효율적인 정책 외 강화 학습 알고리즘을 사용하여 획득 할 수도 있다. 
  
  가능한 경우 사람이 제공한 데모를 통합하면 특히 데모를 수집하기 쉬운 도메인에서 더욱 효율적인 meta learning이 가능하다. 

  메타 테스트 시간에 새로운 작업에 직면했을 때 이 방법은 기존 메타 강화 학습과 동일한 문제를 해결한다. 보상 신호만 사용하여 새로운 기술을 습득하는 것이다. 

  해당 논문의 주요 기여는 감독된 모방을 통해 빠른 강화 학습을 학습하는 meta-RL 방법이다. 하나 또는 몇 개의 그라데이션 단계만 전문가의 조치와 일치하는 정책으로 이어지도록 매개 변수 집합을 최적화한다. 

  감독된 모방은 안정적이고 효율적이기 때문에 우리의 접근 방식은 시각적 제어 영역과 고차원 컨볼루션 네트워크로 우아하게 확장될 수 있다. 
  
  메타 훈련 중에 데모를 사용하면 메타 최적화에서 탐색에 대한 어려움이 줄어들어 희소 보상 환경에서 학습하는 방법을 효과적으로 배울 수 있다. 

  모방과 RL의 조합은 이전에 탐구되었지만 meta learning 맥락에서 모방과 RL의 조합은 이전에 고려되지 않다. 실험에서 알 수 있듯이 이 조합은 실제로 매우 강력하다. 

  meta-RL에 비해 우리의 방법은 상호 작용 에피소드를 최대 10 배까지 줄여 비교 가능한 적응 기술을 메타 학습 할 수 있으므로 실제 학습에서 meta-RL을 훨씬 더 실용적으로 만들 수 있다.

  해당 논문의 실험은 또한 우리의 방법을 통해 몇 번의 경사 하강 단계만으로 시행 착오를 통해 새로운 목표에 컨볼루션 신경망 정책을 적용하고 몇 번의 시도를 통해 희소 보상 조작 작업에 정책을 적용 할 수 있음을 보여준다. 

  해당 논문은 이것이 복잡한 실제 환경에서 사용하기 위해 meta-RL을 실용화하기위한 중요한 단계라고 믿는다.








