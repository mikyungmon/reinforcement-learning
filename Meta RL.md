# Meta-RL # 

**Meta-RL** 이란? 

-> 강화학습 task에 meta learning을 적용한 것이다.

여기서 **meta learning**이란 머신러닝 실험에 대한 메타 데이터에 자동 학습 알고리즘이 적용되는 머신러닝의 하위분야로써, 과거의 학습 경험을 토대로 모델이 새로운 task를 잘 습득할 수 있게 만든 것이다.

예를 들어 머신러닝은 좋은 모델을 만들기 위해서는 일반적으로 수많은 샘플을 활용해 학습시키는 것이 필요하다. 하지만 고양이나 새를 많이 보지 않은 사람이라고 할지라도 그것을 빠르게 구분할 수 있다. 

머신러닝에도 이와 같이 적은 샘플만 가지고도 새로운 개념과 기술을 빠르게 학습하는 것이 가능할까? 이것이 바로 본질적으로 **meta learning**이 풀고하자는 문제점이다.

![image](https://user-images.githubusercontent.com/66320010/117289420-850e2400-aea7-11eb-8e5c-139ecbb19149.png)

- meta learning은 학습을 수행하며 만들어진 학습에 대한 'meta data'를 활용한다.

- 새로운 task에 대한 학습 결과를 예측하고 최적의 'meta data'를 도출하는 것을 목표로 한다.

- meta learning은 3가지로 나눌 수 있다.

  1) Model Evaluation ->  "**다른 task**에 대해 학습시켰을 때의 학습 결과를 토대로 **신규 task**에 대한 결과를 예측하고, 하이퍼파라미터를 추천하자!"
  2) Meta Feature
  3) Prior Model

**Meta-RL은 경험하지 못한 task에 대해서 빠르고 효율적으로 해결할 수 있는 agent를 개발함으로써 "meta learn" 강화학습 task를 해결하는 경우에 대해서 다룬다.**

## 관련 논문 preview ##

**Guided Meta-Policy Search(2020)**

- Abstract

  강화학습 알고리즘은 복잡한 작업에 대해 유망한 결과를 보여 주었지만 처음부터 학습하기 때문에 종종 비실용적인 수의 샘플이 필요하다. 

  Meta-RL은 이전 작업의 경험을 활용하여 새로운 작업을보다 신속하게 해결함으로써 이러한 문제를 해결하는 것을 목표로 한다. 

  그러나 실제로 이러한 알고리즘은 일반적으로 meta learning 과정 중에 많은 양의 정책에 대한(on-policy) 경험을 필요로 하므로 많은 문제에서 사용하기에는 비실용적이다. 

  이를 위해 우리는 정책을 벗어난(off-policy) 개별 학습자가 개별 meta learning 과제를 해결 한 다음, 이러한 솔루션을 단일 meta-learner로 통합 할 수있는 연합 방식으로 강화 학습 절차를 학습 할 것을 제안한다. 

  중앙 meta-learner는 개별 작업에 대한 솔루션을 모방하여 학습하므로 표준 meta RL 문제 설정 또는 일부 또는 모든 작업에 예제 데모가 제공되는 하이브리드 설정을 수용 할 수 있다. 

  전자는 메타 교육 중에 상당한 양의 정책 데이터없이 이전 작업에 대해 학습 한 정책을 활용할 수있는 접근 방식을 제공하는 반면 후자는 데모를 제공하기 쉬운 경우에 특히 유용하다. 

  여러 연속 제어 meta-RL 문제에서, 우리는 이전 작업과 비교하여 meta-RL 샘플 효율성이 현저히 개선되었으며 시각적 관찰을 통해 도메인으로 확장할 수있는 능력을 보여준다. 

1. Introduction










