# Meta-RL # 

**Meta-RL** 이란? 

-> 강화학습 task에 meta learning을 적용한 것이다.

여기서 **meta learning**이란 머신러닝 실험에 대한 메타 데이터에 자동 학습 알고리즘이 적용되는 머신러닝의 하위분야로써, 과거의 학습 경험을 토대로 모델이 새로운 task를 잘 습득할 수 있게 만든 것이다.

예를 들어 머신러닝은 좋은 모델을 만들기 위해서는 일반적으로 수많은 샘플을 활용해 학습시키는 것이 필요하다. 하지만 고양이나 새를 많이 보지 않은 사람이라고 할지라도 그것을 빠르게 구분할 수 있다. 

머신러닝에도 이와 같이 적은 샘플만 가지고도 새로운 개념과 기술을 빠르게 학습하는 것이 가능할까? 이것이 바로 본질적으로 **meta learning**이 풀고하자는 문제점이다.

![image](https://user-images.githubusercontent.com/66320010/117289420-850e2400-aea7-11eb-8e5c-139ecbb19149.png)

- meta learning은 학습을 수행하며 만들어진 학습에 대한 'meta data'를 활용한다.

- 새로운 task에 대한 학습 결과를 예측하고 최적의 'meta data'를 도출하는 것을 목표로 한다.

- meta learning은 3가지로 나눌 수 있다.

  1) Model Evaluation ->  "**다른 task**에 대해 학습시켰을 때의 학습 결과를 토대로 **신규 task**에 대한 결과를 예측하고, 하이퍼파라미터를 추천하자!"
  2) Meta Feature
  3) Prior Model

**Meta-RL은 경험하지 못한 task에 대해서 빠르고 효율적으로 해결할 수 있는 agent를 개발함으로써 "meta learn" 강화학습 task를 해결하는 경우에 대해서 다룬다.**

## 관련 논문 preview ##

**Guided Meta-Policy Search(2020)**   

논문 링크 : https://arxiv.org/pdf/1904.00956.pdf

- **Abstract**

  강화학습 알고리즘은 복잡한 작업에 대해 유망한 결과를 보여 주었지만 처음부터 학습하기 때문에 종종 비실용적인 수의 샘플이 필요하다. 

  Meta-RL은 이전 작업의 경험을 활용하여 새로운 작업을보다 신속하게 해결함으로써 이러한 문제를 해결하는 것을 목표로 한다. 

  그러나 실제로 이러한 알고리즘은 일반적으로 meta learning 과정 중에 많은 양의 정책에 대한(on-policy) 경험을 필요로 하므로 많은 문제에서 사용하기에는 비실용적이다. 

  이를 위해 우리는 정책을 벗어난(off-policy) 개별 학습자가 개별 meta learning 과제를 해결한 다음, 이러한 솔루션을 단일 meta-learner로 통합 할 수 있는 연합 방식으로 강화 학습 절차를 학습 할 것을 제안한다. 

  중앙 meta-learner는 개별 작업에 대한 솔루션을 모방하여 학습하므로 표준 meta RL 문제 설정 또는 일부 또는 모든 작업에 예제 데모가 제공되는 하이브리드 설정을 수용할 수 있다. 

  전자는 meta learning 중에 상당한 양의 정책 데이터없이 이전 작업에 대해 학습한 정책을 활용할 수있는 접근 방식을 제공하는 반면 후자는 데모를 제공하기 쉬운 경우에 특히 유용하다. 

  여러 연속 제어 meta-RL 문제에서, 우리는 이전 작업과 비교하여 meta-RL 샘플 효율성이 현저히 개선되었으며 시각적 관찰을 통해 도메인으로 확장할 수있는 능력을 보여준다. 

- **Introduction**

  **meta learning**은 광범위한 작업에서 이전 경험을 사용하여 새로운 작업의 학습을 크게 가속화 할 수있는 유망한 접근 방식이다. 

  **메타 강화 학습(Meta RL)** 은 이전 경험을 바탕으로 환경과의 상호 작용을 몇 번만 수행하여 시행 착오를 통해 새로운 행동을 학습하는 맥락에서 이 문제를 특별히 고려한다. 

  효과적인 meta-RL 알고리즘을 구축하는 것은 에이전트가 각각의 새로운 객체와 목표에 대해 처음부터 배우지 않고도 새로운 방식으로 새로운 객체를 조작할 수있는 것과 같이 유연한 에이전트를 구축하는데 중요하다. 

  이러한 방식으로 이전 경험을 재사용 할 수 있다는 것은 아마도 지능의 근본적인 측면일 것이다.

  에이전트가 meta-RL을 통해 적응할 수 있도록하는 것은 다양하고 역동적인 환경이 있는 실제 상황에서 행동을 획득할 때 특히 유용하다. 

  그러나 **최근의 발전에도 불구하고 현재의 meta-RL 방법은 상대적으로 낮은 차원의 연속 제어 작업 및 개별 작업 명령을 사용한 탐색과 같은 단순한 영역으로 제한되는 경우가 많다.**  

  최적화 안정성과 샘플 복잡성은 이러한 방법의 meta learning 단계에서 주요 과제이며 일부 최근 기술은 표 MDP에서 meta learning을 위해 최대 2억 5천만번의 전환이 필요하며 이는 일반적으로 1초 이내에 분리하여 해결해야한다. 
  
  해당 논문에서는 이 작업에서 다음과 같은 관찰을 한다. 메타 강화 학습(Meta RL)의 목표는 빠르고 효율적인 강화 학습 절차를 습득하는 것이지만 이러한 절차 자체는 강화 학습을 통해 직접 습득 할 필요가 없다. 

  대신 메타 수준에서 감독을 제공하기 위해 훨씬 더 안정적이고 효율적인 알고리즘을 사용할 수 있다. 

  이 작업에서 우리는 감독된(지도된) 모방 학습을 사용하는 것이 실용적인 선택임을 보여준다. 

  메타 강화 학습 알고리즘은 강화를 통해 작업을 빠르게 학습 할 수 있는 능력을 최적화하는 동시에 전문가 조치의 형태로 메타 학습 중에 보다 직접적인 감독을 받을 수 있다. 

  결정적으로 이러한 전문가 정책은 표준 강화 학습 방법에 의해 자동으로 생성될 수 있으므로 감독에 대한 추가 가정이 실제로 필요하지 않다. 

  메타 강화 학습과 함께 사용하기 어려운 매우 효율적인 정책 외 강화 학습 알고리즘을 사용하여 획득 할 수도 있다. 
  
  가능한 경우 사람이 제공한 데모를 통합하면 특히 데모를 수집하기 쉬운 도메인에서 더욱 효율적인 meta learning이 가능하다. 

  메타 테스트 시간에 새로운 작업에 직면했을 때 이 방법은 기존 메타 강화 학습과 동일한 문제를 해결한다. 보상 신호만 사용하여 새로운 기술을 습득하는 것이다. 

  **해당 논문의 주요 기여는 감독된 모방을 통해 빠른 강화 학습을 학습하는 meta-RL 방법이다. 하나 또는 몇 개의 그라데이션 단계만 전문가의 조치와 일치하는 정책으로 이어지도록 매개 변수 집합을 최적화한다.** 

  감독된 모방은 안정적이고 효율적이기 때문에 우리의 접근 방식은 시각적 제어 영역과 고차원 컨볼루션 네트워크로 우아하게 확장될 수 있다. 
  
  meta learning중에 데모를 사용하면 메타 최적화에서 탐색에 대한 어려움이 줄어들어 희소 보상 환경에서 학습하는 방법을 효과적으로 배울 수 있다. 

  모방과 RL의 조합은 이전에 탐구되었지만 meta learning 맥락에서 모방과 RL의 조합은 이전에 고려되지 않다. 실험에서 알 수 있듯이 이 조합은 실제로 매우 강력하다. 

  meta-RL에 비해 우리의 방법은 상호 작용 에피소드를 최대 10 배까지 줄여 비교 가능한 적응 기술을 메타 학습 할 수 있으므로 실제 학습에서 meta-RL을 훨씬 더 실용적으로 만들 수 있다.

  해당 논문의 실험은 또한 논문의 방법을 통해 몇 번의 경사 하강 단계만으로 시행 착오를 통해 새로운 목표에 컨볼루션 신경망 정책을 적용하고 몇 번의 시도를 통해 희소 보상 조작 작업에 정책을 적용 할 수 있음을 보여준다. 

  해당 논문은 이것이 복잡한 실제 환경에서 사용하기 위해 meta-RL을 실용화하기위한 중요한 단계라고 믿는다.

- **Related Work**

  해당 논문의 작업은 **meta learning**에 대한 사전 작업을 기반으로하며, 여기서 목표는 효율적으로 학습하는 방법을 배우는 것이다. 
  
  해당 논문은 메타 강화 학습의 특정 사례에 초점을 맞춘다. 
  
  이전 방법은 정책에 기울기를 제공하는 학습된 비평가(learned critic) 또는 플래너와 적응형 모델 사용하고 학습된 초기화로부터 기울기 하강을 사용하여 반복적 또는 반복적 신경망으로 표현되는 
  reinforcement learners를 학습했다.
  
  반대로, 우리의 접근 방식은 정책 기울기와 같은 고분산 알고리즘에 의존하는 대신 메타 최적화를 위해 지도 학습을 활용하는 것을 목표로 한다. 
  
  우리는 빠른 RL 절차를 배우는 문제에서 각 작업에 대한 전문가 궤적을 얻는 문제를 분리한다. 
  
  이를 통해 효율적이고 정책을 벗어난 RL 알고리즘을 사용하여 전문가의 궤도를 얻을 수 있다. 
  
  최근 연구에서는 데이터 효율적인 meta learning을 달성하기 위해 상각된 확률적 추론(reinforcement learners)을 사용했지만, 이러한 상황별 방법은 배포 외 테스트 작업에 지속적으로 적응할 수 없다. 
  
  또한 가능한 경우 예제 데모를 활용하는 방법의 기능은 까다로운 희소 보상 작업에서 훨씬 더 나은 성능을 제공한다. 
  
  우리의 접근 방식은 메타 최적화를 위해지도 학습을 활용한다는 점에서 퓨샷 모방과도 관련이 있다. 
  
  그러나 이러한 방법과 달리 우리는 보상만으로 학습 할 수 있고 새로운 작업에 대한 데모가 필요하지 않은 자동 강화 학습자를 기댈 수 있다(?). 
  
  우리의 알고리즘은 먼저 로컬 학습자와 함께 작업을 개별적으로 해결한 다음 중앙 메타 학습자로 통합하여 meta learning을 수행한다. 
  
  이는 지역 학습자를 사용하는 정책 안내 검색과 같은 방법과 유사하다. 
  
  그러나 이러한 선행 방법은 모든 작업을 해결할 수 있는 단일 정책을 학습하는 것을 목표로하지만, 대신 우리의 접근 방식은 교육 작업 배포에 적응할 수있는 단일 학습자를 meta learning하고 그렇지 않은 훈련 중에 볼 수 없었던 새로운 작업에 적응하도록 일반화하는 것을 목표로 한다. 
  
  이전 방법은 또한 단일 작업 설정에서 표준 강화 학습을 보다 효율적으로 만들기 위해 데모를 사용하려고 했다.
  
  이 방법은 RL 문제를 더 쉽게 만들기 위해 데모를 사용하여 데모 및 보상에서 정책을 배우는 것을 목표로 한다.
  
  대신 우리의 접근 방식은 데모를 활용하여 데모없이 새로운 작업을 효율적으로 강화하는 방법을 배우고 시행 착오를 통해서만 새로운 작업을 학습하는 것을 목표로 한다. 
  
  반복을 통해 데이터가 집계되는 알고리즘 버전은 DAgger 알고리즘을 meta learning 설정으로 확장한 것이며이를 통해 성능에 대한 이론적 보장을 제공할 수 있다.
  
  ![image](https://user-images.githubusercontent.com/66320010/117297839-a1af5980-aeb1-11eb-93d9-8efaab4cfb29.png)
  
  [ figure 1: 가이드 메타 정책 검색 알고리즘의 개요. 최적화의 내부 루프에서 강화 학습을 사용하고 메타 최적화에서 지도 학습을 사용하여 강화 학습을 통해 새로운 작업에 빠르게 적응할 수 있는 정책 π_θ 를 학습한다. 이 알고리즘은 작업 별 전문가 π_i^∗를 교육하거나 사람이 시연하여 제공한다고 가정한 다음 이를 메타 최적화에 사용한다. 중요한 것은 새로운 작업에 직면했을 때 단순히 정책 그라데이션을 통해 표준 강화 학습을 수행 할 수 있으며 메타 교육으로 인해 정책이 새로운 작업에 빠르게 적응할 수 있다는 것이다. ]

- **Discussion & Future Work**

  이 연구에서는 **감독된 모방(supervised imitation)을 통해 효율적인 RL 절차를 학습하는 meta-RL 알고리즘을 제시**했다. 

  이를 통해 전문가가 제공한 시연을 통합하는 훨씬 더 효율적인 meta learning 단계를 통해 강화 학습 절차 및 사전 획득을 대폭 가속화 할 수 있다. 

  해당 논문의 방법은 메타 강화 학습의 주요 한계를 해결한다고 믿는다. 

  메타 강화 학습 알고리즘은 몇 개의 샘플만으로 메타 테스트 시간에 새로운 작업을 학습 할 수있는 적응 절차를 효과적으로 획득 할 수 있지만 메타 훈련 중 샘플 수 측면에서 비용이 매우 많이 들어서 실제 문제에 대한 적용 가능성을 제한한다. 

  데모를 통해 meta learning을 가속화함으로써 meta learning 시간과 메타 테스트 시간 모두에서 샘플 효율적인 학습을 가능하게 할 수 있다. 

  감독된 모방의 효율성과 안정성을 고려할 때, 이 연구의 방법이 이미지와 같은 고차원 관찰이있는 영역에 쉽게 적용 할 수있을 것으로 기대된다.

  또한 실험에 필요한 샘플 수를 고려할 때 우리의 접근 방식은 실제 로봇 시스템에서 실행하기에 충분히 효율적일 수 있다. 

  실제 강화 학습에 대한 접근 방식의 적용을 조사하는 것은 향후 작업을위한 흥미로운 방향이다.



















































